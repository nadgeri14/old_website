<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
    <title type="text" xml:lang="en">Personal Page</title>
    <link type="application/atom+xml" rel="self" href="http://localhost:3333/atom.xml"/>
  
  <link href="http://localhost:3333/"/>
  <id>http://localhost:3333/</id>
  <updated>2018-12-09T16:03:17Z</updated>
  <author>
    <name>ABHISHEK</name>
    <email>abhishek22596@gmail.com</email>
  </author>
  <rights type="text">Copyright © 2018 ABHISHEK. All rights reserved.</rights>
  
  <entry>
  <title type="text">Neural Style Transfer</title>
  <link rel="alternate" type="text/html" href="http://localhost:3333/post1.html" />
  <id>http://localhost:3333/post1</id>
  <published>2018-12-04T00:00:00Z</published>
  <updated>2018-12-04T00:00:00Z</updated>
  <content type="html"><![CDATA[ <blockquote>
  <p>Creating art using deep learning.</p>
</blockquote>
<p>Ever wanted to paint like an artist, in this blog we try to achieve that using deep learning. We will use deep learning to compose the images in style of other images.</p>
<p>In this blog we will compose images in style of other. This is known as <b>Neural Transfer</b>. The technique was outlined by <b>Leon A. Gatys</b> in his paper titled <b>"<u> <a href="https://arxiv.org/abs/1508.06576">A Neural Algorithm of Artistic Style</a></u>"</b></p>

<h2>What is Style Transfer?</h2>
<p>It is a technique where you take a 3 images a content image, style image and input image and and blend them together such that input image is transformed to look like the content image, but <b>“painted”</b>  in the style of the style image.</p>

<p><img src="assets/images/style-transfer-demo.png" alt="style-transfer-demo" class="img-responsive" /></p>
<p>As seen the content of the image on left is combined with the style of image on the right. Content refers to the shape and pattern in the image example the outlines whereas style represents the color pattern.</p>
<p>Style transfer is a fun and interesting technique that showcases the capabilities and internal representations of neural networks.</p>

<h2>What is content of an image?</h2>
<p>Convolution Neural Networks are widely popular in the field of Computer Vision. They were inspired by the biological connectivity pattern between the neurons of visual cortex in animals. To learn more about CNNs read the blog by Andrej Karpathy in the course CS231n <a href="http://cs231n.github.io/convolutional-networks/"><u>here</u></a>. </p>
<p>We use a pretrained VGG network as in the paper.</p>
<p>Convolutional Layers are able to capture the various feature ranging from low level features such as edges to high level features such as shapes. So as we go down deep into CNN layers, they learn to extract more and more complex feature from a given image, so the higher layers will discard detailed information about the image, thus have more content information as compared to the lower layers</p>
<p><img src="assets/images/cnn-features.png" alt="cnn-features" class="img-responsive" /></p>
<p>The content reconstruction from left to right of CNN as we go deep into the layers. It can been seen that the deeper layers preserve the higher level representation of the image but loses the details of the image.</p>

<h2>What is style of an image?</h2>
<p>Unlike content of an image, style cannot be directly seen by the looking at feature maps of the CNN layer. But Leon found a way to represent it mathematically. It is defined by calculating the Gram matrix <b>G</b> of a feature map <b>F</b> as :</p>
<p>$$G_{i,j} = \sum_{k}F_{i,k}F_{j,k}$$</p>

<p><img src="assets/images/style-features.png" alt="style-features" class="img-responsive" /></p>
<p>We combine the style from different layers as it captures both the finer texture and larger elements of the original image.</p>

<h2>Loss function</h2>
<p>We have a clear understanding of what is style and content of an image. Now let's define a loss function that tell us how far we are from our style transfer. Let our content image be <img src="https://latex.codecogs.com/gif.latex?C" title="C" class="latex" />, Style image <img src="https://latex.codecogs.com/gif.latex?S" title="S" class="latex" />, and target image <img src="https://latex.codecogs.com/gif.latex?T" title="T" class="latex" />.</p>
<h3>Content Loss :</h3>
<p>Content loss for a layer <img src="https://latex.codecogs.com/gif.latex?l" title="l" class="latex" />  defined as the Euclidean distance between the Feature map <img src="https://latex.codecogs.com/gif.latex?F^{i}" title="F^{i}" class="latex" /> of our content image <img src="https://latex.codecogs.com/gif.latex?C" title="C" class="latex" /> and the feature map <img src="https://latex.codecogs.com/gif.latex?M^{i}" title="M^{i}" class="latex" /> of our target image  <img src="https://latex.codecogs.com/gif.latex?T" title="T" class="latex" /> as </p>
<p>$$L_{content} = \sum_{i,j} (F^{l}_{i,j} - M_{i,j}^{l})^{2}$$</p>
<h3>Style Loss :</h3>
<p>Style loss for a layer  <img src="https://latex.codecogs.com/gif.latex?l" title="l" class="latex" />  is defined as the Euclidean distance between the gram matrix of Feature map <img src="https://latex.codecogs.com/gif.latex?G^{i}" title="G^{i}" class="latex" />  of out Style Image and Feature map <img src="https://latex.codecogs.com/gif.latex?A^{i}" title="A^{i}" class="latex" />of our target image as  
</p>
<p>$$E_{l} = \sum_{i,j} (G^{l}_{i,j} - A_{i,j}^{l})^{2}$$</p>
<p>$$L_{style} = \sum_{l}^{L} w_{l}E_{l}$$</p>
<p>Where <img src="https://latex.codecogs.com/gif.latex?w_{l}" title="w_{l}" class="latex" /> are weighting factors of the contribution of each
layer to the total loss.</p>
<p>We consider style loss for various layers of the network.</p>
<h3>Total Loss :</h3>
<p>It is defined as the weighted sum of both the style loss and content loss.</p>
<p>$$L_{total} = \alpha L_{content} + \beta L_{style}$$</p>

<h3>Note :</h3>
<p>Generally <img src="https://latex.codecogs.com/gif.latex?\beta" title="\beta" class="latex" /> is given higher value as content loss is higher to style loss.  </p>
<h2>Optimisation Method : </h2>
<p>Here we don't actually train a network to do classification or regression. We take advantage of backpropagation to minimize the defined loss to obtain the target image. We start with a random image or a white noise (You can also start with the same content image for convenience ). We then improve the image using back propagation.</p>
<p>In summary we follow the following steps:
	 <ul>
  <li>Pass the content and style image through the network and compute the content and style loss for respective layers. 
</li>
  <li>Compute the total loss.</li>
  <li>Back propagate through the network to determine the gradient of the loss function.</li>
  <li>Take a small step in the negative direction of gradient as we want to minimise our loss function. </li>
  <li>Repeat the process until the loss is below a threshold or we are happy :) </li>
</ul> 
</p>
<h2>Pytorch Implementation</h2>
<p>If you are unfamiliar with pytorch, <a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"><u> here</u> </a> is a short tutorial by pytorch creator <a href="http://soumith.ch/"><u>Soumith Chintala</u></a>.  </p>
<p>I have used google colab as they provide GPU accelerator. The entire code can be found <a href="https://colab.research.google.com/drive/1ho56HbE7T57Dkha7PnAX3U-F3tA2_jSZ"><u>here</u></a> .</p>
<p>We can break down the code into following steps </p>
<ul>
	<li>Read the data.</li>
	<li>Preprocess and visualize the data.</li>
	<li>Create a new model.</li>
	<li>Defining the loss function for the network.</li>
	<li>Optimize the loss.</li>
</ul>
<p>Lets go through each of the steps</p>
<h3>Read the data</h3>
<p>Google collab has upload method that allows you to upload the files. Dont try to upload large files as this tends to be slow.</p>
<pre><code>from google.colab import files
uploaded = files.upload()
</code></pre>
<h3>Preprocess and visualize the data.</h3>
<h4>Preprocess</h4>
<p>We define a function load_image to preprocess the image.</p>
<pre><code>def load_image(img_path, max_size=400, shape=None):
    
    image = Image.open(img_path).convert('RGB')
    if max(image.size) &gt; max_size:
        size = max_size
    else:
        size = max(image.size)
    
    if shape is not None:
        size = shape
        
    in_transform = transforms.Compose([
                        transforms.Resize(size),
                        transforms.ToTensor(),
                        transforms.Normalize((0.485, 0.456, 0.406), 
                                             (0.229, 0.224, 0.225))])
    temp = in_transform(image)
    print 'temp shape ==&gt; ',temp.shape
    image = in_transform(image)[:3,:,:].unsqueeze(0)
    print 'after temp ==&gt; ', temp[:,:,:].shape
    print 'final shape ==&gt; ',image.shape
    
    return image</code></pre>

<h4>Visualize</h4>
<p>We define im_convert function to visualize the data.</p>
<pre><code>def im_convert(tensor):
  image = tensor.to("cpu").clone().detach()
  image = image.numpy()
  image = image.squeeze()
  image = image.transpose(1,2,0)
  image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))
  image = image.clip(0,1)
  return image</code></pre>

<h3>Create a new model</h3>
<p>We use a pretrained VGG 19 model, the same that is used in the paper.</p>
<pre><code>vgg = models.vgg19(pretrained = True).features
for params in vgg.parameters():
  params.requires_grad_(False)
vgg.cuda()</code></pre>

<h3>Defining the loss function for the network and Optimize the loss</h3>
<p>Content loss is calculated for a specific layer. Here we comapre <b>conv4_2</b> from the VGG netowrk of the content and target images.</p>
<p>Style loss can be calculated for multiple layers. From the VGG network we use the following for style loss <ul>
	<li>conv1_1</li>
<li>conv2_1</li>
 <li>conv3_1</li>
 <li>conv4_1</li>
 <li>conv4_2</li>
 <li>conv5_1</li>
</ul></p>
<p>Since calcualting loss is a small step, it is combined along with optimization.</p>
<pre><code>for ii in range(1, steps+1):
    
    # get the features from your target image
    target_features = get_features(target, vgg)
    
    # the content loss
    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)
    
    # the style loss
    # initialize the style loss to 0
    style_loss = 0
    # then add to it for each layer's gram matrix loss
    for layer in style_weights:
        # get the "target" style representation for the layer
        target_feature = target_features[layer]
        target_gram = gram_matrix(target_feature)
        _, d, h, w = target_feature.shape
        # get the "style" style representation
        style_gram = style_grams[layer]
        # the style loss for one layer, weighted appropriately
        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)
        # add to the style loss
        style_loss += layer_style_loss / (d * h * w)
        
    # calculate the *total* loss
        total_loss = content_weight * content_loss + style_weight * style_loss
    
    # update your target image
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()</code></pre>

<h2>Results</h2>

 ]]></content>
</entry>



</feed>
